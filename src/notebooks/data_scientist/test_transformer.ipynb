{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8691b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed86b8",
   "metadata": {},
   "source": [
    "## Look through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce93bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='category'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datapath = 'bbc-text.csv'\n",
    "classification_data= pd.read_csv(datapath)\n",
    "classification_data.groupby(['category']).size().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65aa6c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<CLS> parties warned over  grey vote  political parties cannot afford to take older uk voters for granted in the coming election  says age concern.  a survey for the charity suggests 69% of over-55s say they always vote in a general election compared with just 17% of 18 to 24 year olds. charity boss gordon lishman said if a  decisive blow  was struck at the election it would be by older voters who could be relied on to turn out. a total of 3 028 adults aged 18 or over were interviewed for the study. mr lishman urged the next government to boost state pension.  he also called for measures to combat ageism and build effective public services to  support us all in an ageing society .  older people want to see manifesto commitments that will make a difference to their lives   mr lishman said.  political parties must wake up to the fact that unless they address the demands and concerns of older people they will not keep or attract their vote.  in the survey carried out by icm research  14% of people aged between 18 and 34 said they never voted in general elections. among the over-65s  70% said they would be certain to vote in an immediate election  compared with 39% of people under 55. age concern says the over-55s are  united around  key areas of policy they want the government to focus on. for 57%  pensions and the nhs were key issues  while the economy was important for a third  and tax was a crucial area for 25%. <SEP> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self,data, max_length=512):\n",
    "        self.CLS = '<CLS>'\n",
    "        self.SEP = '<SEP>'\n",
    "        self.PAD = '<PAD>'\n",
    "        self.max_length=max_length\n",
    "        self.dataloader = DataLoader(data, batch_size=1, collate_fn=self.text)\n",
    "        self.vocab = build_vocab_from_iterator(map(self.train, self.dataloader), specials=[self.CLS,self.SEP,self.PAD])\n",
    "                \n",
    "    def train(self,text):\n",
    "        tokenization=text.split(\" \")\n",
    "        tokenization=[self.CLS]+tokenization+[self.SEP]\n",
    "        return tokenization\n",
    "    def text(self,data_point):\n",
    "        x,y=data_point[0]\n",
    "        return y\n",
    "    def tokenize(self, text):\n",
    "        tokenization=text.split(\" \")\n",
    "        if(len(tokenization)>(self.max_length-2)):\n",
    "            tokenization=tokenization[:(self.max_length-2)]\n",
    "            \n",
    "        attention_mask=([1]*(len(tokenization)+2))+([0]*(self.max_length-(len(tokenization)+2)))\n",
    "        tokenization=[self.CLS]+tokenization+[self.SEP]+[self.PAD]*(self.max_length-(len(tokenization)+2))\n",
    "        inputs_ids=[self.vocab[token] for token in tokenization]\n",
    "        Input={'input_ids':torch.LongTensor(inputs_ids),'attention_mask':torch.LongTensor(attention_mask)}\n",
    "        return Input \n",
    "\n",
    "    def de_tokenize(self,tokens):\n",
    "        text=[self.vocab.lookup_token(token) for token in tokens['input_ids']]\n",
    "        return \" \".join(text)\n",
    "tokenizer=Tokenizer(classification_data.values)\n",
    "\n",
    "tokenizer.de_tokenize(tokenizer.tokenize(classification_data.values[1000][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2585c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.labels_dict={label:n for n,label in enumerate(df['category'].unique())}\n",
    "        self.labels = [self.labels_dict[label] for label in df['category']]\n",
    "        self.texts = [tokenizer.tokenize(text) for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6318dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780 222 223\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(classification_data.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(classification_data)), int(.9*len(classification_data))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e8170c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dropout=0.1, d_model=512, n_labels=5, nhead=16, num_encoder_layers=12, dim_feedforward=2048):\n",
    "\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, d_model)\n",
    "        #self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        self.transformer_model = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear = torch.nn.Linear(d_model, n_labels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        embeded=self.embedding(input_id)* math.sqrt(self.d_model)\n",
    "        #input_encoded=self.pos_encoder(embeded)\n",
    "        pooled_output = self.transformer_model(src=embeded)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "    \n",
    "transformer=TransformerClassifier(vocab_size=tokenizer.vocab.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5442457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 89/89 [00:32<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.081         | Train Accuracy:  0.216         | Val Loss:  0.087         | Val Accuracy:  0.216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████████████████▊                       | 41/89 [00:15<00:17,  2.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8l/f9pkmlrd2nzgs6txcq2cf8br0000gn/T/ipykernel_47419/53831056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "LR = 1e-6\n",
    "\n",
    "train, val = Dataset(df_train), Dataset(df_val)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=20, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val, batch_size=20)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(transformer.parameters(), lr= LR)\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    \n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "        \n",
    "        train_label = train_label.to(device)\n",
    "        mask = train_input['attention_mask'].to(device)\n",
    "        input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "        \n",
    "        output=transformer(input_id , mask)\n",
    "        \n",
    "        batch_loss = criterion(output, train_label)\n",
    "        total_loss_train += batch_loss.item()\n",
    "        \n",
    "        acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "        total_acc_train += acc\n",
    "\n",
    "        transformer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for val_input, val_label in val_dataloader:\n",
    "\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_input['attention_mask'].to(device)\n",
    "            input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output= transformer(input_id , mask)\n",
    "\n",
    "            batch_loss = criterion(output, val_label)\n",
    "            total_loss_val += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "            total_acc_val += acc\n",
    "\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(df_train): .3f} \\\n",
    "        | Train Accuracy: {total_acc_train / len(df_train): .3f} \\\n",
    "        | Val Loss: {total_loss_val / len(df_val): .3f} \\\n",
    "        | Val Accuracy: {total_acc_val / len(df_val): .3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ce809",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "__getitem__(self, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2eac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a9f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
